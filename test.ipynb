{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python test for machine-learning\n",
    "\n",
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVR\n",
    "import seaborn as sns\n",
    "import io\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gzip\n",
    "import collections\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases to include\n",
    "* ChEmbl\n",
    "* [DrugDatabank](https://www.drugbank.ca)\n",
    "* [Kegg](https://www.genome.jp/kegg/drug/)\n",
    "* [UnitProtKB](https://www.uniprot.org/help/uniprotkb)\n",
    "* [PubChem](https://pubchem.ncbi.nlm.nih.gov/)\n",
    "* [Drug Product Database](https://health-products.canada.ca/api/documentation/dpd-documentation-en.html#a1)\n",
    "* [PharmGKB](https://www.pharmgkb.org/)\n",
    "* [Therapeutic Targets Database](http://bidd.nus.edu.sg/group/cjttd/)\n",
    "* [ChemSpider](http://www.chemspider.com/)\n",
    "* [ChEbi](https://www.ebi.ac.uk/chebi/)\n",
    "* ZINC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chEmbl information was obtained through a select of the database (into mysql in centOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('product_adme.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Drugbank Import, download from the website the compressed file, see [here](https://www.drugbank.ca/releases/latest)  \n",
    "The gzip format works best, so if in *.zip* format, convert to *.gz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for drug databank\n",
    "def drugbank_import(path, create_alias='N'):\n",
    "    \n",
    "    with gzip.open(path) as xml_file:\n",
    "        tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ns = '{http://www.drugbank.ca}'\n",
    "    calc = \"{ns}calculated-properties/{ns}property\"\n",
    "    exp = \"{ns}experimental-properties/{ns}property\"\n",
    "    extern = \"{ns}external-identifiers/{ns}external-identifier\"\n",
    "    inchikey_template = calc+\"[{ns}kind='InChIKey']/{ns}value\"\n",
    "    inchi_template = calc+\"[{ns}kind='InChI']/{ns}value\"\n",
    "\n",
    "    melt_point_template = exp+\"[{ns}kind='Melting Point']/{ns}value\"\n",
    "    Hydrophobicity_template = exp+\"[{ns}kind='Hydrophobicity']/{ns}value\"\n",
    "    isoelectric_template = exp+\"[{ns}kind='Isoelectric Point']/{ns}value\"\n",
    "    molweight_template = exp+\"[{ns}kind='Molecular Weight']/{ns}value\"\n",
    "    molform_template = exp+\"[{ns}kind='Molecular Formula']/{ns}value\"\n",
    "    logP_template = exp+\"[{ns}kind='logP']/{ns}value\"\n",
    "    logS_template = exp+\"[{ns}kind='logS']/{ns}value\"\n",
    "    boil_template = exp+\"[{ns}kind='Boiling Point']/{ns}value\"\n",
    "    caco_template = exp+\"[{ns}kind='caco2 Permeability']/{ns}value\"\n",
    "    water_exp_template = exp+\"[{ns}kind='Water Solubility']/{ns}value\"\n",
    "    pKa_template = exp+\"[{ns}kind='pKa']/{ns}value\"\n",
    "\n",
    "\n",
    "    psa_template = calc+\"[{ns}kind='Polar Surface Area (PSA)']/{ns}value\"\n",
    "    refr_template = calc+\"[{ns}kind='Refractivity']/{ns}value\"\n",
    "    pola_template = calc+\"[{ns}kind='Polarizability']/{ns}value\"\n",
    "    bioa_template = calc+\"[{ns}kind='Bioavailability']/{ns}value\"\n",
    "    ghose_template = calc+\"[{ns}kind='Ghose Filter']/{ns}value\"\n",
    "    mddr_template = calc+\"[{ns}kind='MDDR-Like Rule']/{ns}value\"\n",
    "\n",
    "    # external identifiers\n",
    "    DPD_template = extern + \\\n",
    "        \"[{ns}resource='Drugs Product Database (DPD)']/{ns}identifier\"\n",
    "    PubChem_template = extern+\"[{ns}resource='PubChem Substance']/{ns}identifier\"\n",
    "    kegg_template = extern+\"[{ns}resource='KEGG Drug']/{ns}identifier\"\n",
    "    GKB_template = extern+\"[{ns}resource='PharmGKB']/{ns}identifier\"\n",
    "    UPKB_template = extern+\"[{ns}resource='UniProtKB']/{ns}identifier\"\n",
    "    TTD_template = extern + \\\n",
    "        \"[{ns}resource='Therapeutic Targets Database']/{ns}identifier\"\n",
    "    wiki_template = extern+\"[{ns}resource='Wikipedia']/{ns}identifier\"\n",
    "    ChEMBL_template = extern+\"[{ns}resource='ChEMBL']/{ns}identifier\"\n",
    "\n",
    "    rows = list()\n",
    "    for i, drug in enumerate(root):\n",
    "        row = collections.OrderedDict()\n",
    "        assert drug.tag == ns + 'drug'\n",
    "        row['type'] = drug.get('type')\n",
    "        row['drugbank_id'] = drug.findtext(ns + \"drugbank-id[@primary='true']\")\n",
    "        row['average-mass'] = drug.findtext(ns + \"average-mass\")\n",
    "        row['monoisotopic-mass'] = drug.findtext(ns + \"monoisotopic-mass\")\n",
    "        row['name'] = drug.findtext(ns + \"name\")\n",
    "        # free text\n",
    "        row['volume-of-distribution'] = drug.findtext(\n",
    "            ns + \"volume-of-distribution\")\n",
    "        row['clearance'] = drug.findtext(ns + \"clearance\")\n",
    "        row['half-life'] = drug.findtext(ns + \"half-life\")\n",
    "        row['toxicity'] = drug.findtext(ns + \"toxicity\")\n",
    "        row['metabolism'] = drug.findtext(ns + \"metabolism\")\n",
    "        row['absorption'] = drug.findtext(ns + \"absorption\")\n",
    "        row['smiles'] = drug.findtext(ns + \"smiles\")\n",
    "        # experimental\n",
    "        row['melting point'] = drug.findtext(melt_point_template.format(ns=ns))\n",
    "        row['Hydrophobicity'] = drug.findtext(\n",
    "            Hydrophobicity_template.format(ns=ns))\n",
    "        row['Isoelectric Point'] = drug.findtext(\n",
    "            isoelectric_template.format(ns=ns))\n",
    "        row['Molecular Weight'] = drug.findtext(molweight_template.format(ns=ns))\n",
    "        row['Molecular Formula'] = drug.findtext(molform_template.format(ns=ns))\n",
    "        row['logP EXP'] = drug.findtext(logS_template.format(ns=ns))\n",
    "        row['logS EXP'] = drug.findtext(melt_point_template.format(ns=ns))\n",
    "        row['pKa EXP'] = drug.findtext(pKa_template.format(ns=ns))\n",
    "        row['Boiling Point'] = drug.findtext(boil_template.format(ns=ns))\n",
    "        row['Caco2 Permeability'] = drug.findtext(caco_template.format(ns=ns))\n",
    "        row['Water Solubility EXP'] = drug.findtext(\n",
    "            water_exp_template.format(ns=ns))\n",
    "        # calculated\n",
    "        row['PSA calc'] = drug.findtext(psa_template.format(ns=ns))\n",
    "        row['Refractivity calc'] = drug.findtext(refr_template.format(ns=ns))\n",
    "        row['Polarizability'] = drug.findtext(pola_template.format(ns=ns))\n",
    "        row['Bioavailability'] = drug.findtext(bioa_template.format(ns=ns))\n",
    "        row['Ghose Filter'] = drug.findtext(ghose_template.format(ns=ns))\n",
    "        row['MDDR-Like Rule'] = drug.findtext(mddr_template.format(ns=ns))\n",
    "        # external\n",
    "        row['Drugs Product Database (DPD)'] = drug.findtext(\n",
    "            DPD_template.format(ns=ns))\n",
    "        row['PubChem Substance'] = drug.findtext(PubChem_template.format(ns=ns))\n",
    "        row['KEGG Drug'] = drug.findtext(kegg_template.format(ns=ns))\n",
    "        row['PharmGKB'] = drug.findtext(GKB_template.format(ns=ns))\n",
    "        row['UniProtKB'] = drug.findtext(UPKB_template.format(ns=ns))\n",
    "        row['Therapeutic Targets Database'] = drug.findtext(\n",
    "            TTD_template.format(ns=ns))\n",
    "        row['Wikipedia'] = drug.findtext(wiki_template.format(ns=ns))\n",
    "        row['ChEMBL'] = drug.findtext(ChEMBL_template.format(ns=ns))\n",
    "\n",
    "        # others\n",
    "        row['groups'] = [group.text for group in\n",
    "                         drug.findall(\"{ns}groups/{ns}group\".format(ns=ns))]\n",
    "        row['atc_codes'] = [code.get('code') for code in\n",
    "                            drug.findall(\"{ns}atc-codes/{ns}atc-code\".format(ns=ns))]\n",
    "        row['categories'] = [x.findtext(ns + 'category') for x in\n",
    "                             drug.findall(\"{ns}categories/{ns}category\".format(ns=ns))]\n",
    "        row['inchi'] = drug.findtext(inchi_template.format(ns=ns))\n",
    "        row['inchikey'] = drug.findtext(inchikey_template.format(ns=ns))\n",
    "        \n",
    "        # Add drug aliases\n",
    "        aliases = {\n",
    "            elem.text for elem in\n",
    "            drug.findall(\"{ns}international-brands/{ns}international-brand\".format(ns=ns)) +\n",
    "            drug.findall(\"{ns}synonyms/{ns}synonym[@language='English']\".format(ns=ns)) +\n",
    "            drug.findall(\"{ns}international-brands/{ns}international-brand\".format(ns=ns)) +\n",
    "            drug.findall(\"{ns}products/{ns}product/{ns}name\".format(ns=ns))\n",
    "\n",
    "        }\n",
    "        aliases.add(row['name'])\n",
    "        row['aliases'] = sorted(aliases)\n",
    "\n",
    "        rows.append(row)\n",
    "        \n",
    "    if create_alias=='Y':\n",
    "        alias_dict = {row['drugbank_id']: row['aliases'] for row in rows}\n",
    "        with open('aliases.json', 'w') as fp:\n",
    "            json.dump(alias_dict, fp, indent=2, sort_keys=True)\n",
    "        \n",
    "\n",
    "\n",
    "    def collapse_list_values(row):\n",
    "        for key, value in row.items():\n",
    "            if isinstance(value, list):\n",
    "                row[key] = '|'.join(value)\n",
    "        return row\n",
    "\n",
    "\n",
    "    rows = list(map(collapse_list_values, rows))\n",
    "\n",
    "    columns = ['drugbank_id', 'name', 'type', 'groups', 'atc_codes',\n",
    "               'categories', 'inchikey', 'inchi', 'average-mass',\n",
    "               'monoisotopic-mass', 'volume-of-distribution', 'clearance', 'half-life',\n",
    "               'toxicity', 'metabolism', 'metabolism', 'absorption', 'smiles',\n",
    "               'melting point', 'logS EXP', 'logP EXP', 'pKa EXP', 'Isoelectric Point', 'Molecular Weight', 'Molecular Formula',\n",
    "               'Hydrophobicity', 'Boiling Point', 'Caco2 Permeability', 'Water Solubility EXP', 'PSA calc', 'Refractivity calc', 'Polarizability', 'Ghose Filter', 'MDDR-Like Rule',\n",
    "               'Drugs Product Database (DPD)', 'PubChem Substance', 'KEGG Drug', 'PharmGKB', 'UniProtKB', 'Therapeutic Targets Database', 'ChEMBL', 'Wikipedia']\n",
    "\n",
    "    drugbank_df = pd.DataFrame.from_dict(rows)[columns]\n",
    "\n",
    "    drugbank_slim_df = drugbank_df[\n",
    "        drugbank_df.groups.map(lambda x: 'approved' in x) &\n",
    "        drugbank_df.inchi.map(lambda x: x is not None) &\n",
    "        drugbank_df.type.map(lambda x: x == 'small molecule')\n",
    "    ]\n",
    "\n",
    "    # write drugbank tsv\n",
    "    #drugbank_df.to_csv('drugbank.csv', sep=\";\", index=False)\n",
    "\n",
    "    #drugbank_slim_df.to_csv('drugbank-slim.csv', sep=\";\", index=False)\n",
    "    return drugbank_df,drugbank_slim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbdata,dbdataslim=drugbank_import('../Drugbank/full database.xml.gz')\n",
    "#dbdata,dbdataslim=drugbank_import('drugbank_all_full_database.xml.zip')\n",
    "\n",
    "dbdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to merge the two sources of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full=pd.merge(data, dbdata, left_on='chembl_id', right_on='ChEMBL', how='outer', suffixes=('_left', '_right'))\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.dtypes\n",
    "data=full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the dataframe created** \n",
    "* evaluating datatypes\n",
    "* evaluating certain columns \n",
    "* filtering data\n",
    "* histogram for numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "data.dtypes\n",
    "#2\n",
    "data['published_type'].describe()\n",
    "#3\n",
    "data[data[\"published_type\"] == \"logP\"]\n",
    "#4\n",
    "data.hist(column=\"standard_value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### para filtrar por count é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_column(df,column,filter=0):\n",
    "    if df[column].dtypes=='object':\n",
    "        eval=df.groupby(column).filter(lambda x: len(x) > filter)\n",
    "        #plot\n",
    "        eval[column].value_counts().plot(kind='bar')\n",
    "        #print list\n",
    "        eval[column].value_counts().index\n",
    "        \n",
    "        info=[#df[column].sum(), # Total sum of the column values\n",
    "             #df[column].mean(), # Mean of the column values\n",
    "             #df[column].median(), # Median of the column values\n",
    "             df[column].nunique(), # Number of unique entries\n",
    "             #df[column].max(), # Maximum of the column values\n",
    "             #df[column].min(), # Minimum of the column values\n",
    "             df[column].isnull().sum(),\n",
    "        df[column].describe()] \n",
    "\n",
    "    else:\n",
    "        eval=df.hist(column=column)\n",
    "        info=[df[column].sum(), # Total sum of the column values\n",
    "             df[column].mean(), # Mean of the column values\n",
    "             df[column].median(), # Median of the column values\n",
    "             df[column].nunique(), # Number of unique entries\n",
    "             df[column].max(), # Maximum of the column values\n",
    "             df[column].min(), # Minimum of the column values\n",
    "             df[column].isnull().sum(),\n",
    "             df[column].describe()] \n",
    "        \n",
    "    return info\n",
    "evaluate_column(data,'Boiling Point',25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teste purposes\n",
    "data_eval=data[data[\"published_type\"] == \"pKa\"]\n",
    "[len(data_eval)*0.8,\n",
    " data_eval['acd_most_apka'].sum(), # Total sum of the column values\n",
    " data_eval['acd_most_apka'].mean(), # Mean of the column values\n",
    " data_eval['acd_most_apka'].median(), # Median of the column values\n",
    " data_eval['acd_most_apka'].nunique(), # Number of unique entries\n",
    " data_eval['acd_most_apka'].max(), # Maximum of the column values\n",
    " data_eval['acd_most_apka'].min(),\n",
    " data_eval['acd_logd'].isna().sum()] # Minimum of the column values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "#### filtering for pka\n",
    "pka_data_cleaning from [here](https://www.kaggle.com/mnoori/feature-selection-for-mlr-with-python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df,null_cutoff,published_type,remove_null='Y'):\n",
    "    \n",
    "    #removing duplicate and unmeaninfull columns\n",
    "    df=df.drop(columns=[\"md\", \"cp\",\"cr\",\"at\",\"aa\",\"molregno.2\",\"molregno.1\",\"doc_id.1\",\"doc_id.2\",\"molregno.3\",\"doc_id.1\",\"doc_id.2\"])\n",
    "    df=df.drop(columns=[\"src_id.1\",\"chembl_id.1\",\"assay_id.1\",\"record_id.1\"])\n",
    "    \n",
    "    #selecting for pka\n",
    "    df=df[df[\"published_type\"] == published_type]\n",
    "\n",
    "    #keeping interesting columns\n",
    "    df=df[[\"max_phase\",\"dosed_ingredient\", \"structure_type\",  \"molecule_type\",\n",
    "    \"oral\", \"parenteral\", \"topical\", \"black_box_warning\",\n",
    "    \"natural_product\", \"first_in_class\", \"chirality\", \"prodrug\",\n",
    "    \"inorganic_flag\", \"usan_year\", \"availability_type\", \"usan_stem\",\n",
    "    \"polymer_flag\", \"usan_substem\", \"usan_stem_definition\",\n",
    "    \"indication_class\", \"withdrawn_flag\", \"withdrawn_year\",\n",
    "    \"withdrawn_country\", \"withdrawn_reason\", \"mw_freebase\",\"alogp\",\"hba\",\n",
    "    \"hbd\", \"psa\", \"rtb\", \"ro3_pass\", \"num_ro5_violations\", \"acd_most_apka\",\n",
    "    \"acd_most_bpka\", \"acd_logp\", \"acd_logd\", \"molecular_species\",\n",
    "    \"full_mwt\", \"aromatic_rings\", \"heavy_atoms\", \"num_alerts\",\n",
    "    \"qed_weighted\", \"mw_monoisotopic\",  \"hba_lipinski\",\n",
    "    \"hbd_lipinski\", \"num_lipinski_ro5_violations\",\"assay_type\", \"relationship_type\",\n",
    "    \"confidence_score\",\"standard_value\",\"half-life\",\"toxicity\",\"metabolism\",\"absorption\",\n",
    "    \"smiles\",\"melting point\",\"logS EXP\",\"logP EXP\",\"pKa EXP\",\"Isoelectric Point\",\n",
    "    \"Molecular Weight\",\"Molecular Formula\",\"Hydrophobicity\",\"Boiling Point\",\"Caco2 Permeability\",\n",
    "    \"Water Solubility EXP\",\"PSA calc\",\"Refractivity calc\",\"Polarizability\",\"Ghose Filter\",\"MDDR-Like Rule\",\n",
    "    \"average-mass\",\"monoisotopic-mass\", \"volume-of-distribution\",\"clearance\"]]\n",
    "    \n",
    "\n",
    "    #removing outlier far greater than average\n",
    "    if published_type in [\"pKa\"]:\n",
    "        df=df[df[\"standard_value\"]<400]\n",
    "    \n",
    "    if remove_null=='N':\n",
    "        null_cutoff=0\n",
    "        \n",
    "    #dropping columns with more than a missing values\n",
    "    #null_values=df.isnull().sum()\n",
    "    #drop_missing_values=null_values[null_values>(null_cutoff*len(df))]\n",
    "    #df=df.drop(drop_missing_values.index, axis=1)    \n",
    "    df = df.dropna(thresh=len(df)-null_cutoff*len(df), axis=1)\n",
    "    \n",
    "    # counting null values in text columns\n",
    "    text_cols_nullcount=df.select_dtypes(include=[\"object\"]).isnull().sum().sort_values(ascending=False)\n",
    "    text_cols_nullcols=text_cols_nullcount.index\n",
    "    for col in text_cols_nullcols:\n",
    "        mostcounts=df[col].value_counts().index.tolist()\n",
    "        df[col]=df[col].fillna(mostcounts[0]) #replacing the missing column in a text with the highest number of values\n",
    "\n",
    "    #missing values in numerical columns \n",
    "    num_cols=df.select_dtypes(include=[\"object\",\"float64\"]).columns #selecting numerical columns\n",
    "    num_null_counts=df[num_cols].isnull().sum().sort_values(ascending=False) #counting null values in columns\n",
    "    num_null_cols=num_null_counts[num_null_counts!=0].index #selecting the ones that have missing values\n",
    "    df=df.fillna(df[num_null_cols].mode().to_dict(orient=\"records\")[0]) #replacing missing with mode\n",
    "    #passing categorical to numerical\n",
    "    df=pd.get_dummies(df, prefix=\"is_\")#add column to name as well\n",
    "\n",
    "    #remove duplicates\n",
    "    df=df.drop_duplicates()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pka_data=data_cleaning(data,0.8,'pKa',remove_null='N')\n",
    "pka_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pka_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logp_data=data_cleaning(data,0.8,'logP')\n",
    "logp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analisado o gráfico em baixo, vemos que não existe correlação forte de de nenhuma coluna com o alvo - **standard_value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logd_data=data_cleaning(df=data,published_type='logD',null_cutoff=0.8)    \n",
    "logd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_correlation(df,target,corr_cutoff):\n",
    "    data_train=df.sample(frac=0.7,random_state=200)\n",
    "    data_test=df.drop(data_train.index)\n",
    "\n",
    "    data_x=df.drop(columns=[target])\n",
    "    data_y=df[target]\n",
    "\n",
    "    data_x_train=data_train.drop(columns=[target])\n",
    "    data_y_train=data_train[target]\n",
    "\n",
    "    data_x_test=data_test.drop(columns=[target])\n",
    "    data_y_test=data_test[target]\n",
    "    \n",
    "    corr=data_train.corr()\n",
    "    #fig,ax=plt.subplots(figsize=(8,6))\n",
    "    #sns.heatmap(corr)\n",
    "    features=''\n",
    "    features_text=''\n",
    "    if len(corr[target].where(lambda x : x.abs()>corr_cutoff).dropna())>1:\n",
    "        features=corr[target].where(lambda x : x.abs()>corr_cutoff).dropna()\n",
    "        features_text=features.index.str.cat(sep=', ')+'\\n'\n",
    "    else:\n",
    "        features='1'\n",
    "        features_text='None'\n",
    "    #print('The features correlated with target above the threshold %s are %s' %(corr_cutoff,features_text))\n",
    "    return len(features)\n",
    "\n",
    "check_correlation(pka_data,'standard_value',0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para retirar colunas com variância abaixo de X, mas devolve um np  \n",
    "além disso, temos variaveis \"booleanas\" o que torna complicado aplicar isto pq a variância não há de ser muito grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "#sel.fit_transform(pka_data_corrected_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def xMlr(df,target,frac=0.7,cv=10):\n",
    "    i=0\n",
    "    mse=0\n",
    "    score=0\n",
    "    while i<cv:\n",
    "        np.random.seed(seed=123)\n",
    "        pka_data_train=df.sample(frac=0.7,random_state=200)\n",
    "        pka_data_test=df.drop(pka_data_train.index)\n",
    "\n",
    "        pka_data_x=df.drop(columns=[target])\n",
    "        pka_data_y=df[target]\n",
    "\n",
    "        pka_data_x_train=df.drop(columns=[target])\n",
    "        pka_data_y_train=df[target]\n",
    "\n",
    "        pka_data_x_test=df.drop(columns=[target])\n",
    "        pka_data_y_test=df[target]\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(pka_data_x_train, pka_data_y_train)\n",
    "        #print(regr.coef_)\n",
    "        mse+=(np.mean((regr.predict( pka_data_x_test)-pka_data_y_test)**2))\n",
    "        score+=regr.score(pka_data_x_test, pka_data_y_test)\n",
    "        i+=1\n",
    "    return mse/cv, score/cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE is mean squared error  \n",
    "Explained variance score:   \n",
    "1 is perfect prediction\n",
    "and 0 means that there is no linear relationship\n",
    "between X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlr_mse,mlr_score=xMlr(pka_data,'standard_value')\n",
    "print(\"RMSE is %s. Score is %s.\" % (mlr_mse, mlr_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xSVR(df,target,frac=0.7,cv=10):\n",
    "    i=0\n",
    "    mse=0\n",
    "    score=0\n",
    "    while i<cv:\n",
    "        np.random.seed(seed=123)\n",
    "        pka_data_train=df.sample(frac=0.7,random_state=200)\n",
    "        pka_data_test=df.drop(pka_data_train.index)\n",
    "\n",
    "        pka_data_x=df.drop(columns=[target])\n",
    "        pka_data_y=df[target]\n",
    "\n",
    "        pka_data_x_train=df.drop(columns=[target])\n",
    "        pka_data_y_train=df[target]\n",
    "\n",
    "        pka_data_x_test=df.drop(columns=[target])\n",
    "        pka_data_y_test=df[target]\n",
    "        clf = SVR(gamma='scale', C=1.0, epsilon=0.1)\n",
    "        clf.fit(pka_data_x_train, pka_data_y_train) \n",
    "        mse+=(np.mean((clf.predict( pka_data_x_test)-pka_data_y_test)**2))\n",
    "        score+=clf.score(pka_data_x_test, pka_data_y_test, sample_weight=None)\n",
    "        i+=1\n",
    "    return mse/cv, score/cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_mse,svr_score=xSVR(pka_data,'standard_value')\n",
    "print(\"MSE is %s. Score is %s.\" % (svr_mse, svr_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_test_train(df,length,cv,null_cutoff,correlation):\n",
    "    result={}\n",
    "    eval_df=df.groupby(\"published_type\").filter(lambda x: len(x) > length)\n",
    "    test_list=eval_df[\"published_type\"].value_counts().index\n",
    "    \n",
    "    for item in test_list:\n",
    "        try:\n",
    "            test_df=data_cleaning(df,null_cutoff,str(item))\n",
    "            if check_correlation(corr_cutoff=correlation,df=test_df,target='standard_value')>1:\n",
    "                result[item]=xMlr(test_df,'standard_value',frac=0.7,cv=cv)\n",
    "        except:\n",
    "            print(item)\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "data_result=evaluation_test_train(data,1000,5,0.8,0.1)\n",
    "print (data_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
